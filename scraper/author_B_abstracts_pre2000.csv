ID,Abstract
128,"This paper studies controlled local replication for hash routing, such as CARP, among a collection of loosely-coupled proxy web cache servers. Hash routing partitions the entire URL space among the shared web caches, creating a single logical cache. Each partition is assigned to a cache server. Duplication of cache contents is eliminated and total incoming traffic to the shared web caches is minimized. Client requests for non-assigned-partition objects are forwarded to sibling caches. However, request forwarding increases not only inter-cache traffic but also cpu utilization, thus slows the client response time. We propose a controlled local replication of non-assigned-partition objects in each cache server to effectively reduce the inter-cache traffic. We use a multiple-exit LRU to implement controlled local replication. Trace-driven simulations are conducted to study the performance impact of local replication. The results show that (1) regardless of cache sizes, with a controlled local replication, the average response time, inter-cache traffic and CPU overhead can be effectively reduced without noticeable increases in incoming traffic; (2) for very large cache sizes, a larger amount of local replication can be allowed to reduce inter-cache traffic without increasing incoming traffic; and (3) local replication is effective even if clients are dynamically assigned to different cache servers."
129,"We study the performance of various run placement policies on disks for the merge phase of concurrent mergesorts using parallel prefetching. The initial sorted runs (input) of a merge and its final sorted run (output) are stored on multiple disks but each run resides only on a single disk. In this paper, we examine through detailed simulations three different run placement policies and the impact of buffer thrashing. The results show that, with buffer thrashing avoidance, the best performance can be achieved by a run placement policy that uses a proper subset of the disks dedicated for writing the output runs while the rest of the disks are used for prefetching the input runs in parallel. However, the proper number of write disks is workload dependent, and if not carefully chosen, it can adversely affect the system performance. In practice, a reasonably good performance can be achieved by a run placement policy that does not place the output run of a merge on any of the disks that store its own input runs but allows the output run to share the same disk with some of the input runs of other merges."
130,"In this paper we introduce a new multidimensional index structure called the S-tree. Such indexes are appropriate for a large variety of pictorial databases such as cartography, satellite and medical images. The S-tree discussed in this paper is similar in flavor to the standard R-tree, but accepts mild imbalance in the resulting tree in return for a significantly reduced area, overlap and perimeter in the resulting minimum bounding rectangles. In fact, the S-tree is defined in terms of a parameter which governs the degree to which this trade-off is allowed. We develop an efficient packing algorithm based on this parameter. We then analyze the S-tree analytically, giving theoretical bounds on the degree of imbalance of the tree. We also analyze the S-tree experimentally. The S-tree does well in two dimensions, and even better in three dimensions. Indeed, the S-tree can be expected to do better still as the dimensionality increases. While the S-tree is extremely effective for static databases, we outline the extension to dynamic databases as well."
131,"In recent years, many data mining methods have been proposed for finding useful and structured information from market basket data. The association rule model was recently proposed in order to discover useful patterns and dependencies in such data. This paper discusses a method for indexing market basket data efficiently for similarity search. The technique is likely to be very useful in applications which utilize the similarity in customer buying behavior in order to make peer recommendations. We propose an index called the signature table, which is very flexible in supporting a wide range of similarity functions. The construction of the index structure is independent of the similarity function, which can be specified at query time. The resulting similarity search algorithm shows excellent scalability with increasing memory availability and database size."
132,"The clustering problem is well known in the database literature for its numerous applications in problems such as customer segmentation, classification and trend analysis. Unfortunately, all known algorithms tend to break down in high dimensional spaces because of the inherent sparsity of the points. In such high dimensional spaces not all dimensions may be relevant to a given cluster. One way of handling this is to pick the closely correlated dimensions and find clusters in the corresponding subspace. Traditional feature selection algorithms attempt to achieve this. The weakness of this approach is that in typical high dimensional data mining applications different sets of points may cluster better for different subsets of dimensions. The number of dimensions in each such cluster-specific subspace may also vary. Hence, it may be impossible to find a single small subset of dimensions for all the clusters. We therefore discuss a generalization of the clustering problem, referred to as the projected clustering problem, in which the subsets of dimensions selected are specific to the clusters themselves. We develop an algorithmic framework for solving the projected clustering problem, and test its performance on synthetic data."
133,"Popular Web sites cannot rely on a single powerful server nor on independent mirrored-servers to support the ever-increasing request load. Distributed Web server architectures that transparently schedule client requests offer a way to meet dynamic scalability and availability requirements. The authors review the state of the art in load balancing techniques on distributed Web-server systems, and analyze the efficiencies and limitations of the various approaches"
134,"In this paper, a hierarchical algorithm, HierarchyScan, is proposed to efficiently locate one-dimensional subsequences within a collection of sequences with arbitrary length. The proposed algorithm performs correlation between the stored sequences and the template pattern in the transformed domain to identify subsequences in a scale- and phase-independent fashion. This is in contrast to those approaches based on the computation of Euclidean distance in the transformed domain. In the proposed hierarchical algorithm, the transformed domain representation of each original sequence is divided into multiple groups of coefficients. The matching is performed hierarchically from the group with the greatest filtering capability to the group with the lowest filtering capability. Only those subsequences whose maximum correlation value is higher than a predefined threshold will be selected for additional screening. This approach is compared to the sequential scanning and an order-of-magnitude speedup is observed."
135,"This paper provides a survey of various data mining techniques for advanced database applications. These include association rule generation, clustering and classification. With the recent increase in large online repositories of information, such techniques have great importance. The focus is on high dimensional data spaces with large volumes of data. The paper discusses past research on the topic and also studies the corresponding algorithms and applications."
136,"Data mining has become increasingly popular and is widely used in various application areas. In this paper, we examine new developments in data mining and its application to personalization in E-commerce. Personalization is what merchants and publishers want to do to tailor the Web site or advertisement and product promotion to a customer based on his past behavior and inference from other like-minded people. E-commerce offers the opportunity to deploy this type of one-to-one marketing instead of the traditional mass marketing. The technology challenges to support personalization will be discussed. These include the need to perform clustering and searching in very high dimensional data space with huge amount of data. We'll examine some of the new data mining technologies developed that can support personalization."
137,"Replication of information across a server cluster provides a promising way to support popular Web sites. However, a Web-server cluster requires some mechanism for the scheduling of requests to the most available server. One common approach is to use the cluster Domain Name System (DNS) as a centralized dispatcher. The main problem is that WWW address caching mechanisms (although reducing network traffic) only let this DNS dispatcher control a very small fraction of the requests reaching the Web-server cluster. The non-uniformity of the load from different client domains, and the high variability of real Web workload introduce additional degrees of complexity to the load balancing issue. These characteristics make existing scheduling algorithms for traditional distributed systems not applicable to control the load of Web-server clusters and motivate the research on entirely new DNS policies that require some system state information. We analyze various DNS dispatching policies under realistic situations where state information needs to be estimated with low computation and communication overhead so as to be applicable to a Web cluster architecture. In a model of realistic scenarios for the Web cluster, a large set of simulation experiments shows that, by incorporating the proposed state estimators into the dispatching policies, the effectiveness of the DNS scheduling algorithms can improve substantially, in particular if compared to the results of DNS algorithms not using adequate state information."
138,"Consider a system of independent tasks to be scheduled without preemption on a parallel computer.  For each task the number of processors required, the execution time, and a weight are known. The problem is to find a schedule with minimum weighted average response time. We present an algorithm called SMART (which stands for scheduling to minimize average response time) for this problem that produces solutions that are within a factor of 8.53 of optimal. To our knowledge this is the first polynomial-time algorithm for the minimum weighted average response time problem that achieves a constant bound. In addition, for the unweighted case (that is, where all the weights are unity) we describe a variant of SMART that produces solutions that are within a factor of 8 of optimal, improving upon the best known bound of 32 for this special case."
139,"With the recent explosion in usage of the World Wide Web, the problem of caching Web objects has gained considerable importance. Caching on the Web differs from traditional caching in several ways. The nonhomogeneity of the object sizes is probably the most important such difference. In this paper, we give an overview of caching policies designed specifically for Web objects and provide a new algorithm of our own. This new algorithm can be regarded as a generalization of the standard LRU algorithm. We examine the performance of this and other Web caching algorithms via event- and trace-driven simulation."
140,"Caching data in a wireless mobile computer can significantly reduce  the bandwidth requirement. However, due to battery power limitation,  a wireless mobile computer may often be forced to operate in a doze or  even totally disconnected mode. As a result, the mobile computer may  miss some cache invalidation reports. In this paper, we present an  energy-efficient cache invalidation method for a wireless mobile  computer. The new cache invalidation scheme is called grouping with  cold update-set retention (GCORE). Upon waking up, a mobile computer  checks its cache validity with the server. To reduce the bandwidth  requirement for validity checking, data objects are partitioned into  groups. However, instead of simply invalidating a group if any of the  objects in the group has been updated, GCORE retains the cold update  set of objects in a group if possible. We present an efficient  implementation of GCORE and conduct simulations to evaluate its  caching effectiveness. The results show that GCORE can substantially  improve mobile caching by reducing the communication bandwidth (thus  energy consumption) for query processing."
141,"The results discussed in this paper are relevant to a large database consisting of consumer profile information together with behavioral (transaction) patterns. The focus of this paper is on the problem of online mining of profile association rules in this large database. The profile association rule problem is closely related to the quantitative association rule problem. We show how to use multidimensional indexing structures in order to perform the mining. The use of multidimensional indexing structures to perform profile mining provides considerable advantages in terms of the ability to perform very generic range based online queries."
142,"A distributed multiserver Web site can provide the scalability necessary to keep up with growing client demand at popular sites. Load balancing of these distributed Web-server systems, consisting of multiple, homogeneous Web servers for document retrieval and a Domain Name Server (DNS) for address resolution, opens interesting new problems. In this paper, we investigate the effects of using a more active DNS which, as an atypical centralized scheduler, applies some scheduling strategy in routing the requests to the most suitable Web server. Unlike traditional parallel/distributed systems in which a centralized scheduler has full control of the system, the DNS controls only a very small fraction of the requests reaching the multiserver Web site. This peculiarity, especially in the presence of highly skewed load, makes it very difficult to achieve acceptable load balancing and avoid overloading some Web servers.This paper adapts traditional scheduling algorithms to the DNS, proposes new policies, and examines their impact under different scenarios. Extensive simulation results show the advantage of strategies that make scheduling decisions on the basis of the domain that originates the client requests and limited server state information (e.g., whether a server is overloaded or not). An initially unexpected result is that using detailed server information, especially based on history, does not seem useful in predicting the future load and can often lead to degraded performance."
143,"With ever increasing Web traffic, a distributed multi-server Web site can provide scalability and flexibility to cope with growing client demands. Load balancing algorithms to spread the requests across multiple Web servers are crucial to achieve the scalability. Various domain name server (DNS) based schedulers have been proposed in the literature, mainly for multiple homogeneous servers. The presence of heterogeneous Web servers not only increases the complexity of the DNS scheduling problem, but also makes previously proposed algorithms for homogeneous distributed systems not directly applicable. This leads us to propose new policies, called adaptive TTL algorithms, that take into account of both the uneven distribution of client request rates and heterogeneity of Web servers to adaptively set the time-to-live (TTL) value for each address mapping request. Extensive simulation results show that these strategies are robust and effective in balancing load among geographically distributed heterogeneous Web servers."
144,"In this paper, we explore a new data mining capability that involves mining path traversal patterns in a distributed information-providing environment where documents or objects are linked together to facilitate interactive access. Our solution procedure consists of two steps. First, we derive an algorithm to convert the original sequence of log data into a set of maximal forward references. By doing so, we can filter out the effect of some backward references, which are mainly made for ease of traveling and concentrate on mining meaningful user access sequences. Second, we derive algorithms to determine the frequent traversal patterns i.e., large reference sequences from the maximal forward references obtained. Two algorithms are devised for determining large reference sequences; one is based on some hashing and pruning techniques, and the other is further improved with the option of determining large reference sequences in batch so as to reduce the number of database scans required. Performance of these two methods is comparatively analyzed. It is shown that the option of selective scan is very advantageous and can lead to prominent performance improvement. Sensitivity analysis on various parameters is conducted."
145,"In a video-on-demand (VOD) environment, diskarrays are often used to support the disk bandwidth requirement. This can pose serious problems on available disk bandwidth upon disk failure. In this paper, we explore the approach of replicating frequently accessed movies to providehigh data bandwidth and fault tolerance required in a disk-array-based video server. An isochronous continuous videostream imposes different requirements from a random accesspattern on databases or files. Explicitly, we propose a newreplica placement method, called <i>rotational mirrored declustering</i> (RMD), to support high data availability for disk arrays in a VOD environment. In essence, RMD is similar tothe conventional mirrored declustering in that replicas arestored in different disk arrays. However, it is different fromthe latter in that the replica placements in different disk arrays under RMD are properly rotated. Combining the meritsof prior chained and mirrored declustering methods, RMD isparticularly suitable for storing multiple movie copies to support VOD applications. To assess the performance of RMD,we conduct a series of experiments by emulating the storage and delivery of movies in a VOD system. Our resultsshow that RMD consistently outperforms the conventionalmethods in terms of load-balancing and fault-tolerance capability after disk failure, and is deemed a viable approachto supporting replica placement in a disk-array-based videoserver."
146,"For a video-on-demand computer system, we propose a scheme which balances the load on the disks, therebyhelping to solve a performance problem crucial to achievingmaximal video throughput. Our load-balancing scheme consists of two components. The static component determinesgood assignments of videos to groups of striped disks. Thedynamic component uses these assignments, and features a"DASD dancing" algorithm which performs real-time diskscheduling in an effective manner. Our scheme works synergistically with disk striping. We examine the performanceof the proposed algorithm via simulation experiments."
147,"With ever increasing web traffic, a distributed Web system can provide scalability and flexibility to cope with growing client demands. Load balancing algorithms to spread the load across multiple Web servers are crucial to achieve the scalability. Various domain name server (DNS) based schedulers have been proposed in the literature, mainly for multiple homogeneous servers. DNS provides (logical) host name to IP-address mapping (i.e., the server assignment), but the mapping is not done for each server access. This is because the address mapping is cached for a time-to-live (TTL) period to reduce network traffic. The presence of heterogeneous Web servers not only increases the complexity of the DNS scheduling problem, but also makes previously proposed algorithms for homogeneous distributed systems such as round robin not directly applicable. This leads us to propose new policies, called adaptive TTL algorithms, that take both the uneven distribution of client request rates and heterogeneity of Web servers into account to adaptively set the TTL value for each address mapping request. Extensive simulation results show that these strategies are effective in balancing load among geographically distributed heterogeneous Web servers."
148,"In this paper, we examine the issue of mining association rules among items in a large database of sales transactions. Mining association rules means that, given a database of sales transactions, to discover all associations among items such that the presence of some items in a transaction will imply the presence of other items in the same transaction. The mining of association rules can be mapped into the problem of discovering large itemsets where a large itemset is a group of items that appear in a sufficient number of transactions. The problem of discovering large itemsets can be solved by constructing a candidate set of itemsets first, and then, identifying within this candidate set those itemsets that meet the large itemset requirement. Generally, this is done iteratively for each large k-itemset in increasing order of k, where a large k-itemset is a large itemset with k items. To determine large itemsets from a huge number of candidate sets in early iterations is usually the dominating factor for the overall data-mining performance. To address this issue, we develop an effective algorithm for the candidate set generation. It is a hash-based algorithm and is especially effective for the generation of candidate set for large 2-itemsets. Explicitly, the number of candidate 2-itemsets generated by the proposed algorithm is, in orders of magnitude, smaller than that by previous methods thus resolving the performance bottleneck. Note that the generation of smaller candidate sets enables us to effectively trim the transaction database size at a much earlier stage of the iterations, thereby reducing the computational cost for later iterations significantly. The advantage of the proposed algorithm also provides us the opportunity of reducing the amount of disk I/O required. Extensive simulation study is conducted to evaluate performance of the proposed algorithm."
149,"In this paper, we explore two important issues, processor allocation and the use of hash filters, to improve the parallel execution of hash joins. To exploit the opportunity of pipelining for hash join execution, a scheme to transform a bushy execution tree to an allocation tree is first devised. In an allocation tree, each node denotes a pipeline. Then, using the concept of synchronous execution time, processors are allocated to the nodes in the allocation tree in such a way that inner relations in a pipeline can be made available at approximately the same time. Also, the approach of hash filtering is investigated to further improve the parallel execution of hash joins. Extensive performance studies are conducted via simulation to demonstrate the importance of processor allocation and to evaluate various schemes using hash filters. It is experimentally shown that processor allocation is, in general ,the dominant factor to performance, and the effect of hash filtering becomes more prominent as the number of relations in a query increases."
150,"In a multinode data sharing environment, different buffer coherency control schemes based on various lock retention mechanisms can be designed to exploit the concept of deferring the propagation or writing of dirty pages to disk to improve normal performance. Two types of deferred write polices are considered. One policy only propagates dirty pages to disk at the times when dirty pages are flushed out of the buffer under LRU buffer replacement. The other policy also performs writes at the times when dirty pages are transferred across nodes. The dirty page propagation policy can have significant implications on the database recovery time. In this paper, we provide an analytical modeling framework for the analysis of the recovery times under the two deferred write policies. We demonstrate how these policies can be mapped onto a unified analytic modeling framework. The main challenge in the analysis is to obtain the pending update count distribution which can be used to determine the average numbers of log records and data I/Os needed to be applied during recovery. The analysis goes beyond previous work on modeling buffer hit probability in a data sharing system where only the average buffer composition, not the distribution, needs to be estimated, and recovery analysis in a single node environment where the complexities on tracking the propagation of dirty pages across nodes and the buffer invalidation effect do not appear. A clipping mechanism can be employed to improve recovery time where the number of pending update on a dirty page is limited by forcing a dirty page to disk after the number of updates accumulated on this page exceeds a certain threshold. The analysis captures the effect of clipping also. Finally, we show the sensitivities of the recovery time and normal performance to the clipping count."
151,"In a Video-on-demand (VOD) computer system, batching requests for the same video to share a common data stream can lead to significant improvement in throughput. Using the {\em wait tolerance} characteristic that is commonly observed in viewers behavior, we introduce a new paradigm for scheduling in VOD systems. We propose and analyze two classes of scheduling schemes: the Max_Batch and Min_Idle schemes, that provide two alternative ways for using a given stream capacity for effective batching. In making a video selection, the proposed schemes take into consideration the next stream completion time as well as the viewer wait tolerance. We compared the proposed schemes with the two previously studied schemes: (1) first-come-first-served (FCFS) that schedules the video with the longest waiting request and (2) the maximum queue length (MQL) scheme that selects the video with the maximum number of waiting requests. We show through simulations that the proposed schemes outperform substantially FCFS and MQL in reducing the viewer turn-away probability, while maintaining a small average response time. In terms of system resources, we show that by exploiting the viewers wait tolerance, the proposed schemes can reduce significantly the server capacity required for achieving a given level of throughput and turn-away probability as compared to the FCFS and MQL. Furthermore, our study shows that an aggressive use of the viewer wait tolerance for batching may not yield the best strategy, and that other factors, such as the resulting response time, fairness, and loss of viewers, should be taken into account."
152,"Energy saving is one of the most important issues in wireless mobile computing. Among others, one viable approach to achieving energy saving is to use an indexed data organization to broadcast data over wireless channels to mobile units. In this paper, we explore the issue of indexing data with skewed access for sequential broadcasting in wireless mobile computing. We propose methods to build index trees based on access frequencies of data records. To minimize the average cost of index probes, we consider two cases: one for fixed index fanouts and the other for variant index fanouts, and devise algorithms to construct index trees for both cases. We show that the cost of index probes can be minimized not only by employing an imbalanced index tree that is designed in accordance with data access skew, but also by exploiting variant fanouts for index nodes."
153,"A distributed Web system, consisting of multiple servers for data retrieval and a Domain Name Server (DNS) for address resolution, can provide the scalability necessary to keep up with growing client demand at popular sites. However, balancing the requests among these atypical distributed servers opens interesting new challenges. Unlike traditional distributed systems in which a centralized scheduler has full control of the system, the DNS controls only a small fraction of the requests reaching the Web site. This makes it very difficult to avoid overloading situations among the multiple Web servers. We adapt traditional scheduling algorithms to the DNS, propose new policies, and examine their impact. Extensive simulation results show the advantage of using strategies that schedule requests on the basis of the origin of the clients and very limited state information, such as whether a server is overloaded or not. Conversely, algorithms that use detailed state information often exhibit the worst performance."
154,"In this paper, we explore an approach of interleaving a bushy execution tree with hash filters to improve the execution of multi-join queries. Similar to semi-joins in distributed query processing, hash filters can be applied to eliminate non-matching tuples from joining relations before the execution of a join, thus reducing the join cost. Note that hash filters built in different execution stages of a bushy tree can have different costs and effects. The effect of hash filters is evaluat ed first. Then, an efficient scheme to determine an effective sequence of hash filters for a bushy execution tree is developed, where hash filters are built and applied based on the join sequence specified in the bushy tree so that not only is the reduction effect optimized but also the cost associated is minimized. Various schemes using hash filters are implemented and evaluated via simulation. It is experimentally shown that the application of hash filters is in general a very powerful means to improve th e execution of multi-join queries, and the improvement becomes more prominent as the number of relations in a query increases."
155,"In this paper, we propose the approach of using multiple hash tables for lock requests with different data access patterns to minimize the number of false contentions in a data sharing environment. We first derive some theoretical results on using multiple hash tables. Then, in light of these derivations, a two-step procedure to design multiple hash tables is developed. In the first step, data items are partitioned into a given number of groups. Each group of data items is associated with the use of a hash table in such a way that lock requests to data items in the same group will be hashed into the same hash table. In the second step, given an aggregate hash table size, the hash table size for each individual data group is optimally determined so as to minimize the number of false contentions. Some design examples and remarks on the proposed method are given. It is observed from real database systems that different data sets usually have their distinct data access patterns, thus resulting in an environment where this approach can offer significant performance improvement."
156,"This paper presents divergence control methods for epsilon serializability (ESR) in centralized databases. ESR alleviates the strictness of serializability (SR) in transaction processing by allowing for limited inconsistency. The bounded inconsistency is automatically maintained by divergence control (DC) methods in a way similar to SR is maintained by concurrency control (CC) mechanisms. However, DC for ESR allows more concurrency than CC for SR. We first demonstrate the feasibility of ESR by showing the design of three representative DC methods: two-phase locking, timestamp ordering and optimistic approaches. DC methods are designed by systematically enhancing CC algorithms in two stages: extension and relaxation. In the extension stage, a CC algorithm is analyzed to locate the places where it identifies non-SR conflicts of database operations. In the relaxation stage, the non-SR conflicts are relaxed to allow for controlled inconsistency. We then demonstrate the applicability of ESR by presenting the design of DC methods using other most known inconsistency specifications, such as absolute value, age and total number of nonserializably read data items. In addition, we present a performance study using an optimistic divergence control algorithm as an example to show that a substantial improvement in concurrency can be achieved in ESR by allowing for a small amount of inconsistency."
157,"Dynamic finite versioning (DFV) schemes are an effective approach to concurrent transaction and query processing, where a finite number of consistent, but maybe slightly out-of-date, logical snapshots of the database can be dynamically derived for query access. In DFV, the storage overhead for keeping additional versions of changed data to support the logical snapshots and the amount of obsolescence faced by queries are two major performance issues. In this paper, we analyze the performance of DFV, with emphasis on the trade-offs between the storage cost and obsolescence. We develop analytical models based on a renewal-process approximation to evaluate the performance of DFV using M 2 snapshots. Asymptotic closed-form results for high query arrival rates are given for the case of two snapshots. Simulation is used to validate the analytical models and to evaluate the trade-offs between various strategies for advancing snapshots when M > 2. The results show that 1) the analytical models match closely with simulation; 2) both the storage cost and obsolescence are sensitive to the snapshot-advancing strategies, especially for M > 2 snapshots; and 3) generally speaking, increasing the number of snapshots demonstrates a trade-off between storage overhead and query obsolescence. For cases with skewed access or low update rates, a moderate increase in the number of snapshots beyond two can substantially reduce the obsolescence, while the storage overhead may increase only slightly, or even decrease in some cases. Such a reduction in obsolescence is more significant as the coefficient of variation of the query length distribution becomes larger. Moreover, for very low update rates, a large number of snapshots can be used to reduce the obsolescence to almost zero without increasing the storage overhead."
158,"Mining information and knowledge from large databases has been recognized by many researchers as a key research topic in database systems and machine learning, and by many industrial companies as an important area with an opportunity of major revenues. Researchers in many different fields have shown great interest in data mining. Several emerging applications in information providing services, such as data warehousing and on-line services over the Internet, also call for various data mining techniques to better understand user behavior, to improve the service provided, and to increase the business opportunities. In response to such a demand, this article is to provide a survey, from a database researcher's point of view, on the data mining techniques developed recently. A classification of the available data mining techniques is provided, and a comparative study of such techniques is presented."
159,"In this paper, we study the subject of exploiting interoperator parallelism to optimize the execution of multi-join queries. Specifically, we focus on two major issues: 1) scheduling the execution sequence of multiple joins within a query, and 2) determining the number of processors to be allocated for the execution of each join operation obtained in 1). For the first issue, we propose and evaluate by simulation several methods to determine the general join sequences, or bushy trees. Despite their simplicity, the heuristics proposed can lead to the general join sequences that significantly outperform the optimal sequential join sequence. The quality of the join sequences obtained by the proposed heuristics is shown to be fairly close to that of the optimal one. For the second issue, it is shown that the processor allocation for exploiting interoperator parallelism is subject to more constraints such as execution dependency and system fragmentation than those in the study of intraoperator parallelism for a single join. The concept of synchronous execution time is proposed to alleviate these constraints. Several heuristics to deal with the processor allocation, categorized by bottom-up and top-down approaches, are derived and are evaluated by simulation. The relationship between issues 1) and 2) is explored. Among all the schemes evaluated, the two-step approach proposed, which first applies the join sequence heuristic to build a bushy tree as if under a single processor system, and then, in light of the concept of synchronous execution time, allocates processors to execute each join in the bushy tree in a top-down manner, emerges as the best solution to minimize the query execution time."
160,"A critical issue in the performance of a video-on-demand system is the I/O bandwidth required in order to satisfy client requests. A number of techniques have been proposed in order to reduce these bandwidth requirements. In this paper we concentrate on one such technique, known as adaptive piggybacking. We develop and analyze piggyback merging policies which are optimal over large classes of reasonable methods."
161,"All-to-all broadcast refers to the process by which every node broadcasts its certain piece of information to all other nodes in the system. In this paper, we develop all-to-all broadcast schemes by dealing with two classes of schemes. A prior scheme based on generation of minimal complete sets is first described, and then a new scheme based on propagation of experts is developed. The former always completes the broadcasting in the minimal number of steps and the latter is designed to minimize the number of messages. Performance of these two classes of schemes is comparatively analyzed. The all-to-all broadcast scheme desired can be derived by combining the advantages of these two classes of schemes."
162,"In this paper, we conduct a performance study of coupling multiple systems with a global buffer, and present several results obtained from a multiple-system simulator. This simulator has been run against three workloads, and the coupled system behavior with these three different inputs is studied. Several statistics, including those on local and global buffer hits, page writes to the global buffer, cross-invalidations, and castouts are reported. Their relationship to the degree of data skew is explored. Moreover, in addition to the update-caching approach, a design alternative for the use of a global buffer, namely read-caching, is explored. In read-caching, not only updated pages but also pages read by each node are kept in the global buffer, thereby facilitating other nodes' access to the same pages at the cost of a higher global buffer usage. Also investigated is the case of no-caching, i.e., without using a global buffer. Several simulation results are presented and analyzed."
163,"A Redundant Array of Independent Disks (RAID) of G disks provides protection against single disk failures by adding one parity block for each G 1 data blocks. In a clustered RAID, the G data/parity blocks are distributed over a cluster of C disks (C > G), thus reducing the additional load on each disk due to a single disk failure. However, most methods proposed for implementing such a mapping do not work for general C and G values.In this paper, we describe a fast mapping algorithm based on almost-random permutations. An analytical model is constructed, based on the queue with a permanent customer, to predict recovery time and read/write performance. The accuracy of the results derived from this model is validated by comparing with simulations. Our analysis shows that clustered RAID is significantly more tolerant of disk failure than the basic RAID scheme. Both recovery time and performance degradation during recovery are substantially reduced in clustered RAID; moreover, these gains can be achieved using fairly small C/G ratios."
164,"Caching can reduce the bandwidth requirement in a mobile computing environment. However, due to battery power limitations, a wireless mobile computer may often be forced to operate in a doze or even totally disconnected mode. As a result, the mobile computer may miss some cache invalidation reports broadcasted by a server, forcing it to discard the entire cache contents after waking up. In this paper, we present an energy-efficient cache invalidation method, called GCORE, that allows a mobile computer to operate in a disconnected mode to save battery while still retaining most of the caching benefits after a reconnection. We present an efficient implementation of GCORE and conduct simulations to evaluate its caching effectiveness. The results show that GCORE can substantially improve mobile caching by reducing the communication bandwidth (or energy consumption) for query processing."
165,"We present a hierarchical algorithm, HierarchyScan, that efficiently locates one-dimensional subsequences within a collection of sequences of arbitrary length. The subsequences identified by HierarchyScan match a given template pattern in a scale- and phase-independent fashion. The idea is to perform correlation between the stored sequences and the template in the transformed domain hierarchically. Only those subsequences whose maximum correlation value is higher than a predefined threshold will be selected. The performance of this approach is compared to the sequential scanning and an order-of-magnitude speedup is observed."
166,"The possibility of fast access to the main memory of remote sites has been advanced as a potential performance improvement in distributed systems. Even if a page is not available in local memory, sites need not do a disk access. Instead, the sites can use efficient mechanisms that support rapid request/response exchanges in order to access pages that are currently buffered at a remote site. Hardware and software support in such a remote caching architecture must also include algorithms that determine which pages should be buffered at what sites. When each site uses the classic LRU replacement algorithm, performance can be much worse than optimal in many system configurations. Because sites do not coordinate individual decisions, overall system buffering/caching decisions yield very inefficient global configurations. This paper proposes an easily implementable modification of the LRU replacement algorithm for LAN environments that reduces replication. The algorithm substantially improves hit-ratios and thus performance over a wide range of parameters. The relatively simple LAN topology implies that much less state information need be available for good replacement decisions compared to general network topologies. Two implications of two variations of the algorithm are explored. In an environment where the network is not a performance bottleneck, and where performance is memory-limited, performance of the proposed replacement algorithm is shown to be close to optimal."
167,"The pipelined execution of multijoin queries in a multiprocessor-based database system is explored in this paper. Using hash-based joins, multiple joins can be pipelined so that the early results from a join, before the whole join is completed, are sent to the next join for processing. The execution of a query is usually denoted by a query execution tree. To improve the execution of pipelined hash joins, an innovative approach on query execution tree selection is proposed to exploit segmented right-deep trees, which are bushy trees of right-deep subtrees. We first derive an analytical model for the execution of a pipeline segment, and then, in light of the model, develop heuristic schemes to determine the query execution plan based on a segmented right-deep tree so that the query can be efficiently executed. As shown by our simulation, the proposed approach, without incurring additional overhead on plan execution, possesses more flexibility in query plan generation, and can lead to query plans of better performance than those achievable by the previous schemes using right-deep trees."
168,"There has been a good deal of progress made recently toward the efficient parallelization of individual phases of single queries in multiprocessor database systems. In this paper we devise and experimentally evaluate a number of scheduling algorithms designed to handle multiple parallel queries. (Scheduling in this context implies the determination of both processor allotments and temporal processor assignments to individual queries and query phases.) One of these algorithms performs the best in our experiments. This algorithm is hierarchical in nature: In the first phase, a good quality precedence-based schedule is created for each individual query and each possible number of processors. This component employs dynamic programming. In the second phase, the results of the first phase are used to create an overall schedule of the full set of queries. This component is based on previously published work on nonprecedence-based malleable scheduling. Even though the problem we are considering is NP-hard in the strong sense, the multiple query schedules generated by our hierarchical algorithm are seen experimentally to achieve high quality results."
169,"In this paper, we examine the issue of mining association rules among items in a large database of sales transactions. The mining of association rules can be mapped into the problem of discovering large itemsets where a large itemset is a group of items which appear in a sufficient number of transactions. The problem of discovering large itemsets can be solved by constructing a candidate set of itemsets first and then, identifying, within this candidate set, those itemsets that meet the large itemset requirement. Generally this is done iteratively for each large k-itemset in increasing order of k where a large k-itemset is a large itemset with k items. To determine large itemsets from a huge number of candidate large itemsets in early iterations is usually the dominating factor for the overall data mining performance. To address this issue, we propose an effective hash-based algorithm for the candidate set generation. Explicitly, the number of candidate 2-itemsets generated by the proposed algorithm is, in orders of magnitude, smaller than that by previous methods, thus resolving the performance bottleneck. Note that the generation of smaller candidate sets enables us to effectively trim the transaction database size at a much earlier stage of the iterations, thereby reducing the computational cost for later iterations significantly. Extensive simulation study is conducted to evaluate performance of the proposed algorithm."
170,"For a video-on-demand computer system we propose a scheme which balances the load on the disks, thereby helping to solve a performance problem crucial to achieving maximal video throughput. Our load balancing scheme consists of two stages. The static stage determines good assignments of videos to groups of striped disks. The dynamic phase uses these assignments, and features a DASD dancing algorithm which performs real-time disk scheduling in an effective manner. Our scheme works synergistically with disk striping. We examine the performance of the DASD dancing algorithm via simulation experiments."
171,"Data replication has been widely used as a means of increasing the data availability for critical applications in the event of disk failure. There are different ways of organizing the two copies of the data across a disk array. This paper compares strategies for striping data of the two copies in the context of database applications. By keeping both copies active, we explore strategies that can take advantage of the additional copy to improve not only availability, but also performance during both normal and failure modes. We consider the effects of small and large stripe sizes on the performance of disk arrays with two active copies of data under a mixed workload of queries and transactions with a skewed access pattern. We propose a dual (hybrid) striping strategy which uses different stripe sizes for the two copies and a disk queuing policy designed to exploit this organization for optimal performance. An analytical model is devised for this scheme, by treating the individual disks as independent, and applying an M/G/1 queuing model. Disks on which a large query scan is running are modeled by a variation of the queue with permanent customers, which leads to an iterative functional equation for the query scan delay distribution. A solution for this equation is given. The results are validated against simulations and are shown to match well. Comparison with uniform striping strategies show that the dual striping scheme yields the most stable performance in a variety of workloads, out-performing the uniform striping strategy using either mirrored or chained declustering under both normal and failure mode operations."
172,"The analytic prediction of buffer hit probability, based on the characterization of database accesses from real reference traces, is extremely useful for workload management and system capacity planning. The knowledge can be helpful for proper allocation of buffer space to various database relations, as well as for the management of buffer space for a mixed transaction and query environment. Access characterization can also be used to predict the buffer invalidation effect in a multi-node environment which, in turn, can influence transaction routing strategies. However, it is a challenge to characterize the database access pattern of a real workload reference trace in a simple manner that can easily be used to compute buffer hit probability. In this article, we use a characterization method that distinguishes three types of access patterns from a trace: (1) locality within a transaction, (2) random accesses by transactions, and (3) sequential accesses by long queries. We then propose a concise way to characterize the access skew across randomly accessed pages by logically grouping the large number of data pages into a small number of partitions such that the frequency of accessing each page within a partition can be treated as equal. Based on this approach, we present a recursive binary partitioning algorithm that can infer the access skew characterization from the buffer hit probabilities for a subset of the buffer sizes. We validate the buffer hit predictions for single and multiple node systems using production database traces. We further show that the proposed approach can predict the buffer hit probability of a composite workload from those of its component files."
173,"In this paper, we examine a set of load sharing strategies that are robust to the unreliable state information that is often present in a distributed database system. In this environment, sites must solve the problem of how alternative sites should be selected to process incoming transactions, given that the information on which the decision is based exhibits varying degrees of obsolescence. A set of regression-based adaptive strategies is examined in which a feedback mechanism is used to compensate for obsolete information. Transaction response time under the different adaptive strategies is evaluated, and the reasons for these performance differences discussed. The key characteristic of the best regression strategy is that transaction site affinity is taken into consideration when adjusting for the effect of information obsolescence."
174,"Broadcast, referring to a process of information dissemination in a distributed systemwhereby a message originating from a certain node is sent to all other nodes in thesystem, is a very important issue in distributed computing. All-to-all broadcast means theprocess by which every node broadcasts its certain piece of information to all othernodes. In this paper, we first develop the optimal all-to-all broadcast scheme for thecase of one-port communication, which means that each node can only send out onemessage in one communication step, and then, extend our results to the case ofmulti-port communication, i.e., k-port communication, meaning that each node can sendout k messages in one communication step. We prove that the proposed schemes areoptimal for the model considered in the sense that they not only require the minimalnumber of communication steps, but also incur the minimal number of messages."
175,"Parallel processing is an attractive option for relational database systems. As in any parallel environment however, load balancing is a critical issue which affects overall performance. Load balancing for one common database operation in particular, the join of two relations, can be severely hampered for conventional parallel algorithms, due to a natural phenomenon known as data skew. In a pair of recent papers (J. Wolf et al., 1993; 1993), we described two new join algorithms designed to address the data skew problem. We propose significant improvements to both algorithms, increasing their effectiveness while simultaneously decreasing their execution times. The paper then focuses on the comparative performance of the improved algorithms and their more conventional counterparts. The new algorithms outperform their more conventional counterparts in the presence of just about any skew at all, dramatically so in cases of high skew."
176,"Epsilon Serializability (ESR) has been proposed to manage and control inconsistency in extending the classic transaction processing. ESR increases system concurrency by tolerating a bounded amount of inconsistency. In this paper, we present multiversion divergence control (mvDC) algorithms that support ESR with not only value but also time fuzziness in multiversion databases. Unlike value fuzziness, accumulating time fuzziness is semantically different. A simple summation of the length of two time intervals may either underestimate the total time fuzziness, resulting in incorrect execution, or overestimate the total time fuzziness, unnecessarily degrading the effectiveness of mvESR. We present a new operation, called TimeUnion, to accurately accumulate the total time fuzziness. Because of the accurate control of time and value fuzziness by the mvDC algorithm, mvESR is very suitable for the use of multiversion databases for real-time applications that may tolerate a limited degree of data inconsistency but prefer more data recency."
177,"Clustering multiple computing nodes has become increasingly popular for reasons of capacity, availability and cost. One approach to clustering is the data sharing approach where a number of loosely coupled nodes share a common database. In this environment, a global shared buffer can be introduced to alleviate the multisystem invalidation effect either as a disk cache or shared intermediate memory. We develop an analytic model to evaluate different shared buffer management policies (SBMPs) which differ in their choice of data granules to be put into the shared buffer. The methodology analyzes all policies using a uniform framework by decomposing the input stream to the shared buffer into multiple (three) component streams based on their effects on the dependency between the private and shared buffer contents. This approach simplifies the problem of analyzing different SBMPs into 1) estimating the rate of each component stream, and 2) evaluating the impact of dependency on each type of component stream and hence the shared buffer hit probability. A detailed simulation model is also developed to validate the analytic model. We also illustrate how the analytic buffer model can be integrated with other system submodels to examine trade-offs between the SBMPs and to estimate optimal shared buffer allocations from a cost-performance point of view."
178,"In a video-on-demand (VOD) system, it is desirable to provide the user with interactive browsing functions such as “fast forward” and “fast backward.” However, these functions usually require a significant amount of additional resources from the VOD system in terms of storage space, retrieval throughput, network bandwidth, etc. Moreover, prevalent video compression techniques such as MPEG impose additional constraints on the process since they introduce inter-frame dependencies. In this paper, we devise methods to support variable rate browsing for MPEG-like video steams and minimize the additional resources required. Specifically, we consider retrieval for a disk-array-based video server and address the problem of distributing the retrieval requests across the disks."
179,"Coupling multiple computing nodes for transaction processing has become increasingly attractive for reasons of capacity, cost, and availability. This paper presents a comparison of robustness (in terms of performance) of three different architectures for transaction processing. In the shared nothing (SN) architecture, neither disks nor memories are shared. In the shared disk (SD) architecture, all disks are accessible from all nodes, whereas in the shared intermediate memory (SIM) architecture, a shared intermediate level of memory is introduced. Coupling multiple nodes inevitably introduces certain interferences and overheads, which take on different forms and magnitudes under the different architectures. Affinity clustering, which attempts to partition the transactions into affinity clusters according to their database reference patterns, can be employed to reduce the coupling degradation under the different architectures, though in different ways. However, the workload may not be partitionable into N affinity clusters of equal size, where N is the number of nodes in the coupled system, so that the load can be evenly spread over all nodes. In addition to balancing the load, we need to maintain a large fraction of data references within the database affiliated with the affinity cluster. These become increasingly harder to achieve for large values of N. In this paper, we examine the impact of affinity on the performance of these three different coupling architectures."
180,"One major drawback of a RAIDS disk array system is that an update to a data block may involve four disk accesses. Such a high overhead is especially undesirable for workloads with a high update rate. In this paper, we present a dynamic parity grouping (DPG) scheme for efficient parity buffering to reduce the write overhead of a RAID-5 system. In DPG, special parity groups are dynamically created for data blocks with high write activity, referred to as the hot data blocks, in addition to default parity groups for the remaining cold data blocks. The parity blocks of the special parity groups are then buffered in the disk controller cache. As a result, the number of disk accesses on a write to a hot data block is reduced to two."
181,"A parallelizable (or malleable) task is one which can be run on an arbitrary number of processors, with a task execution time that depends on the number of processors allotted to it. Consider a system of M independent parallelizable tasks which are to be scheduled without preemption on a parallel computer consisting of P identical processors. For each task, the execution time is a known function of the number of processors allotted to it. The goal is to find (1) for each task i, an allotment of processors β, and (2) overall, a non-preemptive schedule assigning the tasks to the processors which minimizes the average response time of the tasks. Equivalently, we can minimize the flow time which is the sum of the completion times of each of the tasks."
182,"In this paper we study parallel execution of multiple pipelined hash joins. Specifically, we deal with two issues, processor allocation and the use of hash filters, to improve parallel execution of hash joins. We first present a scheme to transform a bushy execution tree to an allocation tree, where each node denotes a pipeline. Then, processors are allocated to the nodes in the allocation tree based on the concept of synchronous execution time such that inner relations (i.e., hash tables) in a pipeline can be made available approximately the same time. In addition, the approach of hash filtering is investigated to further improve the overall performance. Performance studies are conducted via simulation to demonstrate the importance of processor allocation and to evaluate various schemes using hash filters. Simulation results indicate that processor allocation based on the allocation tree significantly outperforms that based on the original bushy tree, and that the effect of hash filtering becomes prominent as the number of relations in a query increases."
183,"There has been a good deal of progress made recently towards the efficient parallelization of individual phases of single queries in multiprocessor database systems. In this paper we devise and evaluate a number of scheduling algorithms designed to handle multiple parallel queries. One of these algorithms emerges as a clear winner. This algorithm  is hierarchical in nature: In the first phase, a good quality precedence-based schedule is created for each individual query and each possible number of processors. This component employs dynamic programming. In the second phase, the results of the first phase are used to create an overall schedule of the full set of queries. This component is based on previously published work on nonprecedence-based malleable scheduling. Even though the  problem we are considering is NP-hard in the strong sense, the multiple query schedules generated by our hierarchical algorithm are seen experimentally to achieve results which are close to optimal."
184,"The performance of a transaction processing system is very sensitive to the buffer hit probability. In a data sharing environment where multiple computing nodes are coupled together with direct access to shared data on disks, buffer coherency needs to be maintained such that if a data granule is updated by a node, the old copies of this granule present in the buffer of other nodes must be invalidated. The buffer invalidation phenomenon reduces the buffer hit probability in a multi-node environment. After the buffer reaches a certain size, the buffer hit probability will remain constant regardless of further increase in buffer size due to the buffer invalidation effect. This puts an upper limit on the achievable buffer hit probability. Thus the selection of appropriate buffer size is  one of the critical issues in a data sharing environment. In this paper, we develop an asymptotic analysis of the Markov model for a buffer in the data sharing environment. Important relations between buffer size, number of nodes, write-probability and the size of the database to the buffer hit probability had been found in all range of system parameters. A simple expression is obtained for the maximum achievable buffer hit probability and also for the maximum usable buffer size. Various properties of the maximum achievable buffer hit probability and usable buffer size are derived for a skewed access workload. The accuracy of the asymptotic method is validated by numerous case studies."
185,"Examines the effect of skewed database access on the transaction response time in a multisystem data sharing environment, where each computing node has access to shared data on disks, and has a local buffer of recently accessed granules. Skewness in data access can increase data contention since most accesses go to few data items. For the same reason, it can also increase the buffer hit probability. We quantify the resultant effect on the transaction response time, which depends not only on the various system parameters but also on the concurrency control (CC) protocol. Furthermore, the CC protocol can give rise to rerun transactions that have different buffer hit probabilities. In a multisystem environment, when a data block gets updated by a system, any copies of that block in other systems' local buffers are invalidated. Combining these effects, we find that higher skew does not necessarily lead to worse performance, and that with skewed access, optimistic CC is more robust than pessimistic CC. Examining the buffer hit probability as a function of the buffer size, we find that the effectiveness of additional buffer allocation can be broken down into multiple regions that depend on the access frequency distribution."
186,"As the demand for high volume transaction processing grows, coupling multiple computing nodes becomes increasingly attractive. This paper presents a comparison on the resilience of the performance to system dynamics of three architectures for transaction processing. In the shared nothing (SN) architecture, neither disks nor memory is shared. In the shared disk (SD) architecture, all disks are accessible to all nodes while in the shared intermediate memory (SIM) architecture, a shared intermediate level of memory is introduced. A transaction processing system needs to be configured with enough capacity to cope with the dynamic variation of load or with a node failure. Three specific scenarios are considered: 1) a sudden surge in load of one transaction class, 2) varying transaction rates for all transaction classes, and 3) failure of a single processing node. We find that the different architectures require different amounts of capacity to be reserved to cope with these dynamic situations. We further show that the data sharingarchitecture, especially in the case with shared intermediate memory, is more resilient tosystem dynamics and requires far less contingency capacity compared to the SNarchitecture."
187,"Semijoin has traditionally been relied upon to reduce the cost of data transmission for distributed query processing. However, judiciously applying join operations as reducers can lead to further reduction in the amount of data transmission required. In view of this fact, we explore the approach of using join operations as reducers in distributed query processing. We first show that the problem of determining a sequence of join operations for a query can be transformed to that of finding a specific type of set of cuts to the corresponding query graph, where a cut to a graph is a partition of nodes in that graph. Then, in light of this concept, we prove that the problem of determining the optimal sequence of join operations for a given query graph is of exponential complexity, thus justifying the necessity of applying heuristic approaches to solve this problem. By mapping the problem of determining a sequence of join reducers into the one of finding a set of cuts, we develop (for tree and general query graphs, respectively) efficient heuristic algorithms to determine a join reducer sequence for distributed query processing. The algorithms developed are based on the concept of divide and conquer and are of polynomial time complexity. Simulation is performed to evaluate these algorithms."
188,"Presents a parallel hash join algorithm that is based on the concept of hierarchicalhashing, to address the problem of data skew. The proposed algorithm splits the usualhash phase into a hash phase and an explicit transfer phase, and adds an extrascheduling phase between these two. During the scheduling phase, a heuristicoptimization algorithm, using the output of the hash phase, attempts to balance the loadacross the multiple processors in the subsequent join phase. The algorithm naturallyidentifies the hash partitions with the largest skew values and splits them as necessary,assigning each of them to an optimal number of processors. Assuming for concreteness aZipf-like distribution of the values in the join column, a join phase which is CPU-bound,and a shared nothing environment, the algorithm is shown to achieve good join phaseload balancing, and to be robust relative to the degree of data skew and the totalnumber of processors. The overall speedup due to this algorithm is compared to someexisting parallel hash join methods. The proposed method does considerably better in high skew situations."
189,"Studies the cache performance in a remote caching architecture. The authors develop aset of distributed object replication policies that are designed to implement differentoptimization goals. Each site is responsible for local cache decisions, and modifies cachecontents in response to decisions made by other sites. The authors use the optimal andgreedy policies as upper and lower bounds, respectively, for performance in thisenvironment. Critical system parameters are identified, and their effect on systemperformance studied. Performance of the distributed algorithms is found to be close tooptimal, while that of the greedy algorithms is far from optimal."
190,"The concurrency control (CC) method employed can be critical to the performance of transaction processing systems. Conventional locking suffers from the blocking phenomenon, where waiting transactions continue to hold locks and block other transactions from progressing. In a high data contention environment, as an increasing number of transactions wait, a larger number of lock requests get blocked and fewer lock requests can get through. The proposed scheme reduces the blocking probability by deferring the blocking behavior of transactions to the later stages of their execution. By properly balancing the blocking and abort effects, the proposed scheme can lead to better performance than either the conventional locking or the optimistic concurrency control (OCC) schemes at all data and resource contention levels. We consider both static and dynamic approaches to determine when to switch from the nonblocking phase to the blocking phase. An analytical model is developed to estimate the performance of this scheme and determine the optimal operating or switching point. The accuracy of the analytic model is validated through a detailed simulation."
191,"The Concurrency Control (CC) scheme employed can profoundly affect the performance of transaction-processing systems. In this paper, a simple unified approximate analysis methodology to model the effect on system performance of data contention under different CC schemes and for different system structures is developed. This paper concentrates on modeling data contention and then, as others have done in other papers, the solutions of the data contention model are coupled with a standard hardware resource contention model through an iteration. The methodology goes beyond previously published methods for analyzing CC schemes in terms of the generality of CC schemes and system structures that are handled. The methodology is applied to analyze the performance of centralized transaction processing systems using various optimistic- and pessimistic-type CC schemes and for both fixed-length and variable-length transactions. The accuracy of the analysis is demonstrated by comparison with simulations. It is also shown how the methodology can be applied to analyze the performance of distributed transaction-processing systems with replicated data."
192,"In this paper, we develop algorithms to achieve optimal processor allocation for pipelined hash joins in a multiprocessor-based database system. A pipeline of hash joins is composed of several stages, each of which is associated with one join operation. The whole pipeline is executed in two phases: (1) the table-building phase, and (2) the tuple-probing phase. We focus on the problem of allocating processors to the stages of a pipeline to minimize the query execution time. We formulate the processor allocation problem as a two-phase mini-max optimization problem, and develop three optimal allocation schemes under three different constraints. The effectiveness of our problem formulation and solution is verified through a detailed tuple-by-tuple simulation of pipelined hash joins. Our solution scheme is general and applicable to any optimal resource allocation problem formulated as a two-phase mini-max problem."
193,"We study the performance of various run-time thrashing control policies for the merge phase of concurrent mergesorts using parallel prefetching, where initial sorted runs are stored on multiple disks and the final sorted run is written back to another dedicated disk. Parallel prefetching via multiple disks can be attractive in reducing the response times for concurrent mergesorts. However, severe thrashing may develop due to imbalances between input and output rates, thus a large number of prefetched pages in the buffer can be replaced before referenced. We evaluate through detailed simulations three run-time thrashing control policies: (a) disabling prefetching, (b) forcing synchronous writes and (c) lowering the prefetch quantity in addition to forcing synchronous writes. The results show that (1) thrashing resulted from parallel prefetching can severely degrade the system response time; (2) though effective in reducing the degree of thrashing, disabling prefetching may worsen the response time since more synchronous reads are needed; (3) forcing synchronous writes can both reduce thrashing and improve the response time; (4) lowering the prefetch quantity in addition to forcing synchronous writes is most effective in reducing thrashing and improving the response time."
194,"The application of a combination of join and semi-join operations to minimize the amount of data transmission required for distributed query processing is discussed. Specifically, two important concepts that occur with the use of join operations as reducers in query processing, namely, gainful semi-joins and pure joint attributes, are used. Some semi-joint, though not profitable themselves, may benefit the execution of subsequent join operations as reducers. Such a semi-join is termed a gainful semi-join. In addition, join attributes that are not part of the output attributes are referred to as pure join attributes. They exploit the usefulness of gainful semi-joins and use the removability of pure join attributes to reduce the amount of data transmission required for query processing. Heuristic searches are developed to determine a sequence of join and semi-join reducers for query processing. Results indicate the importance of the approach to combining joins and semi-joins for distributed query processing."
195,"This paper presents a new formulation of DASD (direct access storage device) disk arm scheduling schemes for multimedia storage management. The formulation, referred to as grouped sweeping scheduling (GSS), provides a framework for minimizing the buffer space required in the retrieval of multimedia streams. In GSS, the set ofn media streams that are to be served concurrently is divided intog groups. Groups are served in fixed order and streams within each group are served in an elevator-like SCAN scheme. Hence, the fixed order (FIFO) and SCAN schemes are special cases of GSS wheng=n andg=1, respectively. In this paper an optimization problem is formulated in which the buffer requirement is minimized with respect to the two design parameters:g and the size of the service unit, i.e. the number of blocks accessed in each service cycle. This formulation initially assumes that all media streams have the same playout requirements. A procedure of complexityO(1) is developed in computing the optimum solution to this problem. The proof of optimality and comparisons between the optimized GSS scheme and FIFO and SCAN are also presented. The paper also discusses the effect of disk arrays in the GSS formulation and issues related to operating GSS in a dynamic setting where streams arrive and depart in random order. Finally, the GSS scheme is extended to support heterogeneous media streams where each stream may have its own playout requirement."
196,"Six buffer coherency policies for a multisystem transaction processing environment are compared. These policies differ in their basic approaches on how and when the invalidated pages are identified or if the updated pages are propagated to the buffers of the remote nodes. They can be classified as detection, notification (of invalid pages), and (update) propagation oriented approaches. The policies trade off CPU overhead of coherency messages with buffer hit probability in different ways, resulting in a tradeoff of response time and maximum throughput. The main contribution is to develop analytical models to predict buffer hit probabilities under various buffer coherency policies assuming the LRU replacement policy and the independent reference model (IRM). The buffer models are validated using simulation models and show excellent agreement. Integrated analytic models capturing buffer hit probability and CPU overhead are developed to predict the overall response times under these coherency policies. The difference in buffer hit probabilities amongst various policies are found to be very sensitive to the skewness of the data access."
197,"In a multi-server file system environment, related files (under the same subdirectory) are grouped together into filesets, and each filesets are assigned to a fixed server. The file operations to each fileset are served by its associated server. However, the dynamic load to various server varies with time and even with the best possible static load assignment, the loads to various servers are unbalanced (particularly in a small time interval). The short term sever load skew implies that extra server capacity has to be provided to satisfy response time requirement. In this paper, we make the observation that a large number of operations in many environments are to the files that are mostly-read and rarely updated. Such operations can be served by any server without incurring a high coherency. We also propose several dynamic load balancing algorithms of various degrees of complexity that achieve high reduction in load kew albeit reducing the server buffer hit probability. It is shown using real workload traces that server load skew can be significantly reduced using the above policies, and there is an inherent trade-off between load balancing and server buffer hit probability. Some variations of the proposed buddy based policies which limit the number of replications of buffer pages seem to provide a good compromise between the two factors."
198,"Data replication has been widely used as a means of increasing the data availability for critical applications in the event of disk failure. This paper compares strategies for organizing the two copies of data in the context of database applications. We consider the effects of small and large stripe sizes on the performance of disk arrays with two active copies of data under a mixed workload of queries and transactions with a skewed access pattern. We propose a dual striping strategy which uses different stripe sizes for the two copies. An analytical performance model is devised for this scheme, and the results are shown to match well iwth simulation data. Comparison with uniform striping strategies shows that the dual striping scheme yields the most stable performance in a variety of workloads, out-performing the uniform striping strategy using either mirrored or chained declustering under both normal and failure mode operations."
199,"A parallel sort-merge-join algorithm which uses a divide-and-conquer approach to address the data skew problem is proposed. The proposed algorithm adds an extra, low-cost scheduling phase to the usual sort, transfer, and join phases. During the schedulingphase, a parallelizable optimization algorithm, using the output of the sort phase,attempts to balance the load across the multiple processors in the subsequent joinphase. The algorithm naturally identifies the largest skew elements, and assigns each ofthem to an optimal number of processors. Assuming a Zipf-like distribution of data skew,the algorithm is demonstrated to achieve very good load balancing for the join phase, andis shown to be very robust relative, among other things, to the degree of data skew andthe total number of processors."
200,"In a multi-query environment, the marginal utilities of allocating additional buffer to the various queries can be vastly different. The conventional approach examines each query in isolation to determine the optimal access plan and the corresponding locality set. This can lead to performance that is far from optimal. As each query can have different access plans with dissimilar locality sets and sensitivities to memory requirement, we employ the concepts of memory consumption and return on consumption (ROC) as the basis for memory allocations. Memory consumption of a query is its space-time product, while ROC is a measure of the effectiveness of response-time reduction through additional memory consumption. A global optimization strategy using simulated annealing is developed, which minimizes the average response over all queries under the constraint that the total memory consumption rate has to be less than the buffer size. It selects the optimal join method and memory allocation for all query types simultaneously. By analyzing the way the optimal strategy makes memory allocations, a heuristic threshold strategy is then proposed. The threshold strategy is based on the concept of ROC. As the memory consumption rate by all queries is limited by the buffer size, the strategy tries to allocate the memory so as to make sure that a certain level of ROC is achieved. A simulation model is developed to demonstrate that the heuristic strategy yields performance that is very close to the optimal strategy and is far superior to the conventional allocation strategy."
201,"A system structure and protocols for improving the performance of a distributed transaction processing system when there is some regional locality of data reference are presented. A distributed computer system is maintained at each region, and a central computer system with a replication of all databases at the distributed sites is introduced. It provides the advantage of distributed systems principally for local transactions, and has the advantage of centralized systems for transactions accessing nonlocal data. Specialized protocols keep the copies at the distributed and centralized systems consistent without incurring the overhead and delay of generalized protocols for fully replicated databases. The advantages achievable through this system structure and the tradeoffs between protocols for concurrency and coherency control of the duplicate copies of the databases are studied. An approximate analytic model is used to estimate the system performance. It is found that the performance is sensitive to the protocol and that substantial performance improvement can be obtained as compared with distributed systems."
202,"The problem of combining join and semijoin reducers for distributed query processing is studied. An approach based on interleaving a join sequence with beneficial semijoins is proposed. A join sequence is mapped into a join sequence tree first. The join sequence tree provides an efficient way to identify for each semijoin its correlated semijoins as well as its reducible relations under the join sequence. In light of these properties, an algorithm for determining an effective sequence of join and semijoin reducers is developed. Examples are given to illustrate the results. They show the advantage of using a combination of joins and semijoins as reducers for distributed query processing."
203,"The knowledge of access skew (non-uniform access) in each database relation is useful for both workload management (buffer pool allocation, transaction routing, etc.), as well as capacity planning for changing workload mix. However, it is a challenging problem to characterize the access skew of a real database workload in a simple manner that can easily be used to compute the buffer hit probability under the LRU replacement policy. A concise way to characterize the access skew is proposed by assuming that the large number of data pages may be logically grouped into a small number of partitions such that the frequency of accessing each page within a partition can be treated as equal. Based on this approach, a recursive binary partitioning algorithm is presented that can infer the access skew from the buffer hit probabilities for a subset of the buffer sizes. This avoids explicit estimation of individual access frequencies for the large number of database pages. The method is validated of its ability to predict buffer hit from the skew characterization using production database traces."
204,"In this paper we formulate the following natural multiprocessor scheduling problem: Consider a parallel system with P processors. Suppose that there are Ntasks to be scheduled on this system, and that the execution time of each task j ε {1,…,N} is a nonincreasing function tj(βj) of the number of processors βj ε {1,…,P} allotted to it. The goal is to find, for each task j, an allotment of processors βj, and, overall, a schedule assigning the tasks to the processors which minimizes the makespan, or latest task completion time. The so-called shelf strategy is commonly used for orthogonal rectangle packing, a related and classic optimization problem. The prime difference between the orthogonal rectangle problem and our own is that in our case the rectangles are, in some sense, malleable: The height of each rectangle is a nonincreasing function of its width. In this paper, we solve our multiprocessor scheduling problem exactly in the context of a shelf-based paradigm. The algorithm we give uses techniques from resource allocation theory and employs a variety of other combinatorial optimization techniques."
205,"In this paper, we analyze the performance of dynamic finite versioning (DFV) schemes for concurrent transaction and query processing, where a finite number of consistent snapshots can be derived for query access. We develop analytical models based on a renewal process approximation to evaluate the performance of DFV using M ≥ 2 snapshots. The storage overhead and obsolescence faced by queries are measured. Simulation is used to validate the analytical models and to evaluate the trade-offs between various starategies for advancing snapshots when M > 2."
206,"Buffer coherency control can be achieved through retaining a lock (shared, exclusive, etc.) on each page in the buffer, even after the requesting transaction has committed. Depending upon the lock mode held for retention and the compatibility of lock modes specified, different retention policies can be devised. In addition to tracking the validity of the buffered data granules, additional capabilities can be provided such as deferred writes to support no-force policy on commit, (node) location identification of valid granules to support remote memory accesses, and shared/exclusive lock retention to reduce the number of global lock requests for concurrency control. However, these can have serious implications not only on the performance but also on the recovery complexity. In this paper, five different integrated coherency policies are considered. We classify these policies into three different categories according to their recovery requirements. A performance study based on analytic models is provided to understand the trade-offs on both maximum throughputs and response times of the policies with a similar level of recovery complexity and the performance gain achievable through increasing the level of recovery complexity."
207,"A relational database workload analyzer (REDWAR) is developed to characterize the workload in a DB2 environment. This is applied to study a production DB2 system where a structured query language (SQL) trace for a two-hour interval and an image copy of the database catalog were obtained. The results of the workload study are summarized. The structure and complexity of SQL statements, the makeup and run-time behavior of transactions/queries, and the composition of relations and views are discussed. The results obtained provide the important information needed to build a benchmark workload to evaluate the alternative design tradeoffs of database systems."
208,"Analytical models are developed to study hybrid CC (concurrency control) schemes which employ a different CC scheme to handle rerun transactions, since their characteristics are different from the first run of transactions. These include switching to static or dynamic locking during rerun (referred to as static and dynamic hybrid OCC (optimistic concurrency control) schemes, respectively), and switching to broadcast OCC during rerun, while doing pure OCC for the first run. In a high data contention environment where locking is inferior to OCC, analysis shows that the performance can be substantially improved by using this hybrid approach and the authors study the tradeoff of the different hybrid CC schemes. The analytic models are based on a decomposition approach and use a mean-value-type analysis. The accuracy of the analysis is validated through simulations."
209," Broadcasting, which refers to a process of information dissemination in a distributed system whereby a message originating from a certain node is sent to all other nodes in the system, is a very important issue in distributed computing. All-to-all broadcasting means the process by which  every node  broadcasts its certain piece of information to all other nodes. In this paper, we develop  optimal all-to-all broadcasting schemes  for a distributed system of an arbitrary number of nodes to complete the broadcasting with not only the minimal number of communication steps but also the minimal number of messages. We develop the optimal all-to-all broadcasting scheme for the case of k-port communication, meaning that each node can send out k messages in one communication step where k is a positive integer depending on the system. It is shown that the proposed scheme not only requires the minimal number of communication steps but also incurs the minimal number of messages. "
210," The high performance networks in modern distributed systems enable a site to obtain cached objects from the main memory of other sites more quickly han the time needed to access local disks. In this environment, efficient mechanisms can be devised to support rapid request/response exchanges for objects that reside on remote sites. As a result, it becomes possible to use remote memory as an additional layer in the memory hierarchy between local memory and disks. "
211," Parallel processing is an attractive option for relational database systems. As in any parallel environment, however, load balancing is a critical issue which affects overall performance. Load balancing for one common database operation in particular, the join of two relations, can be severely hampered for conventional parallel algorithms, due to a natural phenomenon known as data skew. This is a problem which grows with the number of processors, since even modest skew can cause load imbalances for large numbers of processors. In a pair of recent papers we described two new join algorithms designed to address the data skew problem. These algorithms were based, respectively, on the traditional sort merge and hash join algorithms, and employed techniques borrowed from mathematical optimization theory. In this paper we propose significant improvements to both algorithms, increasing their effectiveness while simultaneously decreasing their execution times. The paper then focuses on the comparative performance of the improved algorithms and their more conventional sort merge and hash counterparts. The latter two are perfectly good algorithms  except  that they fail to deal with data skew. We have run literally thousands of experiments on these four algorithms, across a comprehensive range of data skew distributions, numbers of processos, and performance / configuration parameters. We have examined both I/O- and CPU-bound configurations. The new algorithms outperform their more conventional counterparts in the presence of just about any skew at all, dramatically so in cases of high skew. Even in cases of zero skew, the new algorithms perform nearly as well as the conventional algorithms, indicating their robustness. In particular, the proposed sort merge algorithm is extraordinarily well-behaved, providing near-linear speedup in  every  single experiment we have performed. Contrary to the usual uniprocessor situation, the perform "
212,"An examination is made of a technique to derive the time-stamp or interval of time-stamps dynamically by using limited time-stamp history information to re-order transaction at commit time and to derive a back-shifted time-stamp for certification to reduce read-write conflicts. An analytic model to quantify the performance improvement by comparison with the basic time-stamp certification scheme is developed. Since the committed conflicting transaction may itself be back-shifted, the analytic model needs to estimate the distribution of the back-shift of the certification time-stamp in order to determine the probability of successfully back-shifting a transaction requesting commit. It is shown how this relatively complex protocol can be analyzed using a few simple approximations, validate the analysis through simulations, and the range of parameter values for which this approach is most beneficial is determined."
213,"In this paper we examine the issue of robust transaction routing in a locally distributed database environment where transaction characteristics such as reference locality imply that certain processing systems can be identified as being more suitable than others for a given transaction class. A response time based routing strategy can strike a balance between indiscriminate sharing of the load and routing based only on transaction affinity. Since response time estimates depend on workload and system parameters that may not be readily available, it is important to examine the robustness of routing decisions to information accuracy. We find that a strategy which strictly tries to minimize the response time of incoming transactions is sensitive to the accuracy of certain parameter  values. On the other hand, naive strategies, that simply ignore the parameters in making routing decisions, have even worse performance. Three alternative strategies are therefore examined: threshold, discriminatory, and adaptive. Instead of just optimizing an incoming transaction's response time, the first two strategies pursue a strategy that is somewhat more oriented towards global optimization.  This is achieved by being more restrictive on either the condition or the candidate for balancing the load. The third strategy, while trying to minimize the response time of individual incoming transactions, employs a feedback process to adaptively adjust future response time estimates. It monitors the discrepancy between the actual and estimated response times and introduces a correction  factor based on regression analysis. All three strategies are shown to be robust with respect to the accuracy of workload and system parameters used in the response time estimation."
214,"In a data sharing environment, where a number of loosely coupled computing nodes share a common storage subsystem, the effectiveness of a private buffer at each node is limited due to the multi-system invalidation effect, particularly under a non-uniform data access pattern. A global shared buffer can be introduced to alleviate this problem either as a disk cache or shared memory. In this paper we developed an approximate analytic model to evaluate different shared buffer management policies (SBMPs) which differ in their choice of data granules to be put into the shared buffer. The analytic model can be used to study the trade-offs of different SBMPs and the impact of different buffer allocations between shared and private buffers. The effects of various parameters, such as, the probability of update, the number of nodes, the sizes of private and shared buffer, etc., on the performance of SBMPS are captured in the analytic model. A detailed simulation model is also developed to validate the analytic model. We show that dependency between the contents of the private and shared buffers can play an important role in determining the effectiveness of the shared buffer particularly for a small number of nodes."
215,"The effectiveness of parallel processing of relational join operations is examined. The skew in the distribution of join attribute values and the stochastic nature of the task processing times are identified as the major factors that can affect the effective exploitation of parallelism. Expressions for the execution time of parallel hash join and semijoin are derived and their effectiveness analyzed. When many small processors are used in the parallel architecture, the skew can result in some processors becoming sources of bottleneck while other processors are being underutilized. Even in the absence of skew, the variations in the processing times of the parallel tasks belonging to a query can lead to high task synchronization delay and impact the maximum speedup achievable through parallel execution. For example, when the task processing time on each processor is exponential with the same mean, the speedup is proportional to P/ln(P) where P is the number of processors. Other factors such as memory size, communication bandwidth, etc., can lead to even lower speedup. These are quantified using analytical models."
216,"A hybrid system structure comprised of distributed systems to take advantage of locality of reference and a central system to handle transactions that access non-local data is examined. Several transaction processing applications, such as reservation systems, insurance and banking have such regional locality of reference. A concurrency and coherency control protocol that maintains the integrity of the data and performs well for transactions that access local or non-local data is described. It is shown that the performance of the hybrid system is much less sensitive to the fraction of remote accesses than the distributed system and offers similar performance to the distributed system for local transactions."
217,"Semijoin has traditionally been relied upon for reducing the communication cost required for distributed query processing. However, judiciously applying join operations as reducers can lead to further reduction in the communication cost. In view of this fact, we explore in this paper the approach of using join operations, in addition to semijoins, as reducers in distributed query processing. We first show that the problem of determining a sequence of join operations for a query graph can be transformed to that of finding a set of cuts to that graph, where a cut to a graph is a partition of the nodes in that graph. In light of the mapping we develop an efficient heuristic algorithm to determine an effective sequence of join reducers for a query. The algorithm using the concept of divide-and-conquer is shown to have polynomial time complexity. Examples are also given to illustrate our results."
218,"Parallel processing of relational queries has received considerable attention of late. However, in the presence of data skew, the speedup from conventional parallel join algorithms can be very limited, due to load imbalances among the various processors. Even a single large skew element can cause a processor to become overloaded. In this paper, we propose a parallel sort merge join algorithm which uses a divide-and-conquer approach to address the data skew problem. The proposed algorithm adds an extra scheduling phase to the usual sort, transfer and join phases. During the scheduling phase, a parallelizable optimization algorithm, using the output of the sort phase, attempts to balance the load across the multiple processors in the subsequent join phase. The algorithm naturally identifies the largest skew elements, and assigns each of them to an optimal number of processors. Assuming a Zipf-like distribution for data skew, the algorithm is demonstrated to achieve very good load balancing for the join phase in a CPU-bound environment, and is shown to be very robust relative to the degree of data skew and the total number of processors."
219,"The authors develop an approximate analytical model to study the tradeoffs of replicating data in a distributed database environment. Several concurrency control protocols are considered, including pessimistic, optimistic, and semi-optimistic protocols. The approximate analysis captures the effect of the protocol on hardware resource contention and data contention. The accuracy of the approximation is validated through detailed simulations. It is found that the benefit of replicating data and the optimal number of replicates are sensitive to the concurrency control protocol. Under the optimistic and semi-optimistic protocols, replications can significantly improve response time with an additional MIPS (million instructions per second) requirement to maintain consistency among the replicates. The optimal degree of replication is further affected by the transaction mix (e.g. the fraction of read-only transactions), the communications delay and overhead, the number of distributed sites, and the available MIPS. Sensitivity analyses have been carried out to examine how the optimal degree of replication changes with respect to these factors."
220,"Vertical partitioning can be used to enhance the performance of relational database systems by reducing the number of disk accesses. The authors identify the key parameters for capturing the behavior of an access plan and propose a two-step methodology consisting of a query analysis step to estimate the parameters and a binary partitioning step which can be applied recursively. The partitioning uses an integer linear programming technique to minimize the number of disk accesses. Significant performance benefit would be achieved for join if the partitioned (inner) relation could fit into the memory buffer under the inner-outer loop join method, or if the partitioned relation could fit into the sort buffer under the sort-merge join method, but not the original relation. For cases where a segment scan or a cluster index scan is used, vertical partitioning of the relation with the algorithm described is still often found to lead to substantial performance improvement."
221,"In a distributed database environment, the site assignment of relations is a critical issue. When the joint operations in a query involve relations over multiple sites, the site to carry out the joint operation can have a significant impact on the performance. Based on the query descriptions and arrival frequency to each site, a methodology is developed to assign relations and determine joint sites simultaneously. The methodology first decomposes queries into relation steps and then makes site assignments based on either a linear integer programming technique to minimize the amount of intersystem communication while balancing resource utilizations across systems, or a heuristic technique to minimize average response time under similar resource constraints."
222,"A hybrid architecture is proposed that combines the approaches of a multisystem partitioned database system and a data-sharing multisystem approach offering the advantages of each. With this architecture some databases are shared between systems, while others are retained private by specific systems. The authors examine how to determine which databases to share, which to retain private, and how to route transactions and partition the private databases among systems so as to minimize response time or overheads while balancing the load among systems. A simulated annealing heuristic is used to solve this optimizing problem. Trace data from large mainframe systems running IBM's Information Management System database management system are used to illustrate the methodology and to demonstrate the advantages of the hybrid approach."
223,"The authors propose an integrated control mechanism and analyze the performance gain due to its use. An extension to the data sharing system structure is examined in which a shared intermediate memory is used for buffering and for early commit processing. Read-write-synchronization and write-serialization problems arise. The authors show how the integrated concurrency protocol can be used to overcome both problems. A queueing model is used to quantify the performance improvement. Although using intermediate memory as a buffering device produces a moderate performance benefit, the analysis shows that more substantial gains can be realized when this technique is combined with the use of an integrated concurrency-coherency control protocol."
224,"The authors investigate dynamic transaction routing strategies for locally distributed database systems in which the database is partitioned and distributed among multiple transaction-processing systems, and the incoming transactions are routed by a common front-end processor. If a transaction issues a database request referencing a nonlocal database partition, the request has to be shipped to the system owing the referenced partition for processing. Various dynamic strategies are studied. Their performance is compared with that of the optimal static strategy. A class of dynamic transaction routing strategies which take into account routing history and minimize the estimated response time of incoming transactions is proposed; they are found to provide a substantial improvement over the optimal static strategy. The robustness of the strategies is further studied through sensitivity analysis over various transaction loads, communication overheads, and database reference distributions."
225,"A method for determining the optimal testing period and measuring the production yield is discussed. With the increased complexity of VLSI circuits, testing has become more costly and time-consuming. The design of a testing strategy, which is specified by the testing period based on the coverage function of the testing algorithm, involves trading off the cost of testing and the penalty of passing a bad chip as good. The optimal testing period is first derived, assuming the production yield is known. Since the yield may not be known a priori, an optimal sequential testing strategy which estimates the yield based on ongoing testing results, which in turn determines the optimal testing period, is developed next. Finally, the optimal sequential testing strategy for batches in which N chips are tested simultaneously is presented. The results are of use whether the yield stays constant or varies from one manufacturing run to another."
226,"A methodology is developed to determine the number of processors needed to satisfy transaction throughput and response time requirements for processors of different MIPS (sizes). The minimum MIPS per processor required to satisfy response time and throughput constraints in a transaction processing complex of N coupled systems is also determined. For realistic overhead assumptions, despite large assumed cost advantages on a per-MIPS basis, it is found that very small systems may not match up to the cost/performance of some larger systems, when required to meet the same throughput and response-time constraints. If transactions running on smaller systems were allowed a larger response-time constraint, then it may be possible to construct a lower-cost system from smaller and less expensive processors, generally with lower supportable maximum throughput. Besides the coupling degradation between multiprocessor systems, there is a small systems effect. The cost criterion indicates that there is an optimum processor size below which total system costs would increase appreciably"
227,"The purpose of testing is to determine the correctness of the unit under test in come optimal way. One difficulty in meeting the optimality requirement is that the stochastic properties of the unit are usually unknown a priori. For instance, one might not know exactly the yield of a VLSI production line before one tests the chips made as a result. Given the probability of unit failure and the coverage of a test, the optimal test period is easy to obtain. However, the probability of failure is not usually known a priori. We there- fore develop an optimal sequential testing strategy which estimates the production yield based on ongoing test results, and then use it to determine the optimal test period."
228,"The demand for on-line transaction processing has grown rapidly in recent years. To meet the transaction demand, several DB (database management) and DC (data communication management) subsystems can be coupled together to form a distributed DB/DC system. A key problem is to provide these distributed systems with effective means to recover transactions upon failure while paying little performance penalty during normal processing. Also, there should be minimal interference of fault-free components, during the recovery of failed component. By decentralizing recovery management, and using transaction level structural information to eliminate costly lower level handshaking protocols, proposed progressive transaction recovery protocols seek to solve the problem. A queueing model for evaluating the transaction response time during normal processing for the progressive and pessimistic protocols is developed and solved, via simulation. The progressive recovery protocols are shown to reduce normal processing overhead and lead to performance improvement over the pessimistic protocol."
229,"The prospect of coupling a large number of small inexpensive microprocessor based systems to deliver the performance of a large transaction processing system at lower cost has not been realized, to date. Inter-system interference, multi-system coupling protocol overhead and the increased processing time for smaller systems can cause considerable degradation. A methodology is developed to determine the number of processors needed to satisfy transaction throughput and response time requirements for processors of different MIPS (sizes). The minimum MIPS per processor required to satisfy response time, throughput and utilization constraints in a transaction processing complex of N coupled systems is also determined, by using an approximate analytical model driven by measured workload parameters. Despite large assumed cost advantages on a per MIPS basis we find that small systems do not match up to the cost/performance of some larger systems. Besides multi-system's coupling degradation, there is a small system effect. Because of the increased transaction execution time in smaller systems, transaction hold on to resources longer, thereby causing increased inter-system interference. Our cost criterion indicates that there is an optimum processor size below which total system costs would increase appreciably. Ways to reduce the inter-system interference and coupling protocol overheads are investigated and shown to shift this optimum."
230,"In a multisystem database system with a function request shipping approach, the databases are partitioned among the multiple systems and a facility is provided to support the shipping of database requests among the systems. This is in contrast to a data sharing multisystem approach in which all systems have direct access to the shared database. The performance of the function request shipping approach is examined and compared to the data sharing approach. The emphasis is on generic issues that affect the function shipping approach. Trace data from large mainframe systems running IBM's IMS database management system are used to illustrate the issues of partitioning. A methodology is presented for partitioning the databases and routing transactions among the systems so as to minimize the fraction of remote function calls, while balancing the load among systems. Estimates of the resulting remote function calls, mirror transaction setups, and multisystem two-phase commits are obtained. A simulation model and approximate analysis are used to examine the overall system performance, to study the effect of various workload parameters and design alternatives, and to compare the function shipping and data sharing approaches."
231,"The performance of multiple systems sharing a common data base is analyzed for an architecture with concurrency control using a centralized lock engine. The workload is based on traces from large mainframe systems running IBM's IMS database management system. Based on IMS lock traces the lock contention probability and data base buffer invalidation effect in a multi-system environment is predicted. Workload parameters are generated for use in event-driven simulation models that examine the overall performance of multi-system data sharing, and to determine the performance impact of various system parameters and design alternatives. While performance results are presented for realistic system parameters, the emphasis is on the methodology, approximate analysis technique and on examining the factors that affect multi-system performance."
